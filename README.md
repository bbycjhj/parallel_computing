# parallel_computing

## 1. 实验要求

- 在你自己的机器上,分别使用MPI, OpenMP, Pthread, ISPC各种编程环境对下列算法进行编程 ,且分别在不同问题规模(尺寸)下比较的其性能差 别: 
  - 积分求Pi, n个小区间块
  - 尺寸为n×n的矩阵的转置
  - 尺寸为n×n的矩阵的乘法
- 提交你的代码及实验报告。
- 报告中要说明你的机器的详细配置。
- 报告中要给出实验数据及其分析。

## 2. 实验环境

![配置](D:\文档\多核程序设计\18s.Multi-core\project1\%E9%85%8D%E7%BD%AE1.png)

**CPU**：

```
Intel(R) Core(TM) i7-4710MQ CPU @ 2.50GHz

基准速度:	2.50 GHz
插槽:	1
内核:	4
逻辑处理器:	8
虚拟化:	已启用
L1 缓存:	256 KB
L2 缓存:	1.0 MB
L3 缓存:	6.0 MB
```

## 3. 实验结果

### 积分求Pi

通过将积分图像切分成若干个小的区间块，并计算每一小块的面积，将小块的面积求和以计算积分图像的面积，从而得到Pi的值。每个小块的面积通过并行来计算，从而大幅提高计算效率。

本次实验分别将分割的区间块数设为1000000, 5000000, 10000000, 50000000, 100000000, 500000000, 1000000000，以比较不同规模下ISPC, mpi, openMP 以及pthread的效率。

![计算Pi](https://github.com/bbycjhj/parallel_computing/imgs/计算Pi.png)

由实验结果可知，在计算规模较小时，几种编程方式的效率差距不大，但随着计算规模的增大，openMP的效果显得更好，mpi与openMP不相上下，ISPC的效果则稍差，效果最差的是pthread。在计算Pi的程序中，对进程间的通信需求并不高，因此，mpi在进程间通信方面的开销较小，具有较好的效果。pthread和ISPC看起来在此条件下的表现并不优秀。

### 矩阵转置

将矩阵按行切分成子矩阵，分别分给子进程进行转置运算，最后合并在一起得到完整的转置矩阵。

实验分别设置了500, 1000, 1500, 3000, 5000, 8000, 15000 共7个不同大小的矩阵进行测试。

![矩阵转置](https://github.com/bbycjhj/parallel_computing/imgs/%E7%9F%A9%E9%98%B5%E8%BD%AC%E7%BD%AE.png)

由实验结果可以看出，在矩阵大小较大时，ISPC取得了最好的计算效果，其次是openMP，然后是pthread，最差的是mpi。在矩阵转置的并行运算中，进程间需要进行较大规模的矩阵传递，因此使用消息传递架构的mpi在进行大规模的矩阵运算时效果很差。ISPC充分使用了核内单元，在进行大量简单运算时效率很高。

### 矩阵乘法

将矩阵按行切分，每一行由一个子进程进行乘法运算，最后子进程的计算结果汇总，得到完整的结果矩阵。

实验设置了维数分别为50, 100, 150, 300, 500, 800, 1200, 2000的矩阵进行测试。

![矩阵乘法](https://github.com/bbycjhj/parallel_computing/imgs/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95.png)

由结果可知，效果最好的是ISPC，其次是openMP和pthread，最差的是mpi。与矩阵转置实验类似，ISPC在进行大规模矩阵运算时效果很好，而mpi则因为大量而频繁的进程间通信导致效率低下。openMP和pthread效果一般，不相上下。

